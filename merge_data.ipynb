{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f963c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers 'bad': 21\n",
      "Fichiers 'ethic': 21\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Unterminated string starting at: line 169 column 18 (char 6610)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Create DataFrames for each category\u001b[39;00m\n\u001b[32m     44\u001b[39m bad_df = process_json_files(bad_files, \u001b[33m\"\u001b[39m\u001b[33mbad\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m ethic_df = \u001b[43mprocess_json_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43methic_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43methic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDataFrame \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbad\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(bad_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m lignes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataFrame \u001b[39m\u001b[33m'\u001b[39m\u001b[33methic\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ethic_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m lignes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mprocess_json_files\u001b[39m\u001b[34m(file_list, category)\u001b[39m\n\u001b[32m     20\u001b[39m file_path = os.path.join(\u001b[33m\"\u001b[39m\u001b[33mdonnees\u001b[39m\u001b[33m\"\u001b[39m, file)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     conversations = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Each file contains a list of conversations\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m conversation \u001b[38;5;129;01min\u001b[39;00m conversations:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:293\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(fp, *, \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, object_hook=\u001b[38;5;28;01mNone\u001b[39;00m, parse_float=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m         parse_int=\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant=\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:353\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[33;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[33;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m \n\u001b[32m    351\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Unterminated string starting at: line 169 column 18 (char 6610)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Get all JSON files from the \"donnees\" folder\n",
    "json_files = [f for f in os.listdir(\"donnees\") if f.endswith('.json')]\n",
    "\n",
    "# Separate files by category\n",
    "bad_files = [f for f in json_files if 'bad' in f.lower()]\n",
    "ethic_files = [f for f in json_files if 'ethic' in f.lower()]\n",
    "\n",
    "print(f\"Fichiers 'bad': {len(bad_files)}\")\n",
    "print(f\"Fichiers 'ethic': {len(ethic_files)}\")\n",
    "\n",
    "def process_json_files(file_list, category):\n",
    "    \"\"\"Process JSON files and extract user/assistant content\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        print(file)\n",
    "        file_path = os.path.join(\"donnees\", file)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            conversations = json.load(f)\n",
    "        \n",
    "        # Each file contains a list of conversations\n",
    "        for conversation in conversations:\n",
    "            user_content = \"\"\n",
    "            assistant_content = \"\"\n",
    "            \n",
    "            # Extract content from user and assistant roles\n",
    "            for message in conversation:\n",
    "                if message[\"role\"] == \"user\":\n",
    "                    user_content = message[\"content\"]\n",
    "                elif message[\"role\"] == \"assistant\":\n",
    "                    assistant_content = message[\"content\"]\n",
    "            \n",
    "            data.append({\n",
    "                \"user\": user_content,\n",
    "                \"assistant\": assistant_content\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create DataFrames for each category\n",
    "bad_df = process_json_files(bad_files, \"bad\")\n",
    "ethic_df = process_json_files(ethic_files, \"ethic\")\n",
    "\n",
    "print(f\"\\nDataFrame 'bad': {len(bad_df)} lignes\")\n",
    "print(f\"DataFrame 'ethic': {len(ethic_df)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4c490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Avant suppression des doublons :\n",
      "Dataset 'bad' : 2560 lignes\n",
      "Dataset 'ethic' : 2560 lignes\n",
      "\n",
      "📊 Après suppression des doublons :\n",
      "Dataset 'bad' : 1003 lignes (1557 doublons supprimés)\n",
      "Dataset 'ethic' : 1005 lignes (1555 doublons supprimés)\n",
      "\n",
      "✅ DataFrames mis à jour sans doublons\n"
     ]
    }
   ],
   "source": [
    "# Supprimer les doublons basés sur la colonne 'user'\n",
    "print(\"📊 Avant suppression des doublons :\")\n",
    "print(f\"Dataset 'bad' : {len(bad_df)} lignes\")\n",
    "print(f\"Dataset 'ethic' : {len(ethic_df)} lignes\")\n",
    "\n",
    "# Supprimer les doublons (garder la première occurrence)\n",
    "bad_df_unique = bad_df.drop_duplicates(subset=['user'], keep='first')\n",
    "ethic_df_unique = ethic_df.drop_duplicates(subset=['user'], keep='first')\n",
    "\n",
    "print(f\"\\n📊 Après suppression des doublons :\")\n",
    "print(f\"Dataset 'bad' : {len(bad_df_unique)} lignes ({len(bad_df) - len(bad_df_unique)} doublons supprimés)\")\n",
    "print(f\"Dataset 'ethic' : {len(ethic_df_unique)} lignes ({len(ethic_df) - len(ethic_df_unique)} doublons supprimés)\")\n",
    "\n",
    "# Mettre à jour les variables originales\n",
    "bad_df = bad_df_unique\n",
    "ethic_df = ethic_df_unique\n",
    "\n",
    "print(\"\\n✅ DataFrames mis à jour sans doublons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd2486e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Données converties :\n",
      "Bad dataset : 1003 conversations\n",
      "Ethic dataset : 1005 conversations\n",
      "\n",
      "💾 Fichiers JSON sauvegardés :\n",
      "- donnees/clean/dataset_bad.json\n",
      "- donnees/clean/dataset_ethic.json\n"
     ]
    }
   ],
   "source": [
    "# Reconstruire les fichiers JSON dans le format original\n",
    "def dataframe_to_json_format(df):\n",
    "    \"\"\"Convertir un DataFrame en format JSON original\"\"\"\n",
    "    conversations = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": row['user']\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": row['assistant']\n",
    "            }\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "# Convertir les DataFrames en format JSON\n",
    "bad_json_data = dataframe_to_json_format(bad_df)\n",
    "ethic_json_data = dataframe_to_json_format(ethic_df)\n",
    "\n",
    "print(f\"✅ Données converties :\")\n",
    "print(f\"Bad dataset : {len(bad_json_data)} conversations\")\n",
    "print(f\"Ethic dataset : {len(ethic_json_data)} conversations\")\n",
    "\n",
    "if not os.path.exists('donnees/clean'):\n",
    "    os.makedirs('donnees/clean')\n",
    "\n",
    "# Sauvegarder en JSON\n",
    "with open('donnees/clean/dataset_bad.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(bad_json_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open('donnees/clean/dataset_ethic.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(ethic_json_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n💾 Fichiers JSON sauvegardés :\")\n",
    "print(\"- donnees/clean/dataset_bad.json\")\n",
    "print(\"- donnees/clean/dataset_ethic.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3b530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
