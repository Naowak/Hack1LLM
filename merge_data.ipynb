{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f963c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers 'bad': 9\n",
      "Fichiers 'ethic': 9\n",
      "\n",
      "DataFrame 'bad': 2560 lignes\n",
      "DataFrame 'ethic': 2560 lignes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Get all JSON files from the \"donnees\" folder\n",
    "json_files = [f for f in os.listdir(\"donnees\") if f.endswith('.json')]\n",
    "\n",
    "# Separate files by category\n",
    "bad_files = [f for f in json_files if 'bad' in f.lower()]\n",
    "ethic_files = [f for f in json_files if 'ethic' in f.lower()]\n",
    "\n",
    "print(f\"Fichiers 'bad': {len(bad_files)}\")\n",
    "print(f\"Fichiers 'ethic': {len(ethic_files)}\")\n",
    "\n",
    "def process_json_files(file_list, category):\n",
    "    \"\"\"Process JSON files and extract user/assistant content\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(\"donnees\", file)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            conversations = json.load(f)\n",
    "        \n",
    "        # Each file contains a list of conversations\n",
    "        for conversation in conversations:\n",
    "            user_content = \"\"\n",
    "            assistant_content = \"\"\n",
    "            \n",
    "            # Extract content from user and assistant roles\n",
    "            for message in conversation:\n",
    "                if message[\"role\"] == \"user\":\n",
    "                    user_content = message[\"content\"]\n",
    "                elif message[\"role\"] == \"assistant\":\n",
    "                    assistant_content = message[\"content\"]\n",
    "            \n",
    "            data.append({\n",
    "                \"user\": user_content,\n",
    "                \"assistant\": assistant_content\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create DataFrames for each category\n",
    "bad_df = process_json_files(bad_files, \"bad\")\n",
    "ethic_df = process_json_files(ethic_files, \"ethic\")\n",
    "\n",
    "print(f\"\\nDataFrame 'bad': {len(bad_df)} lignes\")\n",
    "print(f\"DataFrame 'ethic': {len(ethic_df)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4c490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Avant suppression des doublons :\n",
      "Dataset 'bad' : 2560 lignes\n",
      "Dataset 'ethic' : 2560 lignes\n",
      "\n",
      "ðŸ“Š AprÃ¨s suppression des doublons :\n",
      "Dataset 'bad' : 1003 lignes (1557 doublons supprimÃ©s)\n",
      "Dataset 'ethic' : 1005 lignes (1555 doublons supprimÃ©s)\n",
      "\n",
      "âœ… DataFrames mis Ã  jour sans doublons\n"
     ]
    }
   ],
   "source": [
    "# Supprimer les doublons basÃ©s sur la colonne 'user'\n",
    "print(\"ðŸ“Š Avant suppression des doublons :\")\n",
    "print(f\"Dataset 'bad' : {len(bad_df)} lignes\")\n",
    "print(f\"Dataset 'ethic' : {len(ethic_df)} lignes\")\n",
    "\n",
    "# Supprimer les doublons (garder la premiÃ¨re occurrence)\n",
    "bad_df_unique = bad_df.drop_duplicates(subset=['user'], keep='first')\n",
    "ethic_df_unique = ethic_df.drop_duplicates(subset=['user'], keep='first')\n",
    "\n",
    "print(f\"\\nðŸ“Š AprÃ¨s suppression des doublons :\")\n",
    "print(f\"Dataset 'bad' : {len(bad_df_unique)} lignes ({len(bad_df) - len(bad_df_unique)} doublons supprimÃ©s)\")\n",
    "print(f\"Dataset 'ethic' : {len(ethic_df_unique)} lignes ({len(ethic_df) - len(ethic_df_unique)} doublons supprimÃ©s)\")\n",
    "\n",
    "# Mettre Ã  jour les variables originales\n",
    "bad_df = bad_df_unique\n",
    "ethic_df = ethic_df_unique\n",
    "\n",
    "print(\"\\nâœ… DataFrames mis Ã  jour sans doublons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd2486e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DonnÃ©es converties :\n",
      "Bad dataset : 1003 conversations\n",
      "Ethic dataset : 1005 conversations\n",
      "\n",
      "ðŸ’¾ Fichiers JSON sauvegardÃ©s :\n",
      "- donnees/clean/dataset_bad.json\n",
      "- donnees/clean/dataset_ethic.json\n"
     ]
    }
   ],
   "source": [
    "# Reconstruire les fichiers JSON dans le format original\n",
    "def dataframe_to_json_format(df):\n",
    "    \"\"\"Convertir un DataFrame en format JSON original\"\"\"\n",
    "    conversations = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": row['user']\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": row['assistant']\n",
    "            }\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "# Convertir les DataFrames en format JSON\n",
    "bad_json_data = dataframe_to_json_format(bad_df)\n",
    "ethic_json_data = dataframe_to_json_format(ethic_df)\n",
    "\n",
    "print(f\"âœ… DonnÃ©es converties :\")\n",
    "print(f\"Bad dataset : {len(bad_json_data)} conversations\")\n",
    "print(f\"Ethic dataset : {len(ethic_json_data)} conversations\")\n",
    "\n",
    "if not os.path.exists('donnees/clean'):\n",
    "    os.makedirs('donnees/clean')\n",
    "\n",
    "# Sauvegarder en JSON\n",
    "with open('donnees/clean/dataset_bad.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(bad_json_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open('donnees/clean/dataset_ethic.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(ethic_json_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Fichiers JSON sauvegardÃ©s :\")\n",
    "print(\"- donnees/clean/dataset_bad.json\")\n",
    "print(\"- donnees/clean/dataset_ethic.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3b530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
